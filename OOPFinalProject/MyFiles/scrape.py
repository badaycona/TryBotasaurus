import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
from urllib.parse import urljoin
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import os

class CybeneticsPSUScraper:
    def __init__(self, use_selenium=True):
        self.base_url = "https://www.cybenetics.com"
        self.database_url = "https://www.cybenetics.com/index.php?option=psu-performance-database"
        self.use_selenium = use_selenium
        
        if use_selenium:
            self.setup_selenium()
        else:
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            })
    
    def setup_selenium(self):
        """C√†i ƒë·∫∑t Selenium WebDriver"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # Ch·∫°y kh√¥ng hi·ªán giao di·ªán
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            print("ƒê√£ kh·ªüi t·∫°o Chrome WebDriver th√†nh c√¥ng")
        except Exception as e:
            print(f"L·ªói kh·ªüi t·∫°o WebDriver: {e}")
            print("Vui l√≤ng c√†i ƒë·∫∑t ChromeDriver ho·∫∑c s·ª≠ d·ª•ng requests thay th·∫ø")
            self.use_selenium = False
    
    def get_page_content_selenium(self, url):
        """L·∫•y n·ªôi dung trang web b·∫±ng Selenium"""
        try:
            self.driver.get(url)
            
            # ƒê·ª£i trang load
            WebDriverWait(self.driver, 30).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ƒê·ª£i th√™m ƒë·ªÉ JavaScript load xong
            time.sleep(5)
            
            # T√¨m c√°c element c√≥ th·ªÉ ch·ª©a d·ªØ li·ªáu PSU
            print("ƒêang t√¨m ki·∫øm c√°c element ch·ª©a d·ªØ li·ªáu...")
            
            # Ki·ªÉm tra c√°c selector kh·∫£ dƒ©
            possible_selectors = [
                'table',
                '.table',
                '#psu-table',
                '.psu-list',
                '.database-table',
                '[class*="table"]',
                '[class*="grid"]',
                '[class*="list"]',
                'div[data-table]',
                '.data-table'
            ]
            
            elements_found = {}
            for selector in possible_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        elements_found[selector] = len(elements)
                        print(f"T√¨m th·∫•y {len(elements)} element v·ªõi selector: {selector}")
                except:
                    continue
            
            # In ra HTML c·ªßa m·ªôt ph·∫ßn trang ƒë·ªÉ debug
            try:
                body = self.driver.find_element(By.TAG_NAME, "body")
                html_snippet = body.get_attribute('innerHTML')[:2000]  # L·∫•y 2000 k√Ω t·ª± ƒë·∫ßu
                print("\n--- HTML snippet ---")
                print(html_snippet)
                print("--- End snippet ---\n")
            except:
                pass
            
            return self.driver.page_source
            
        except TimeoutException:
            print("Timeout khi load trang")
            return None
        except Exception as e:
            print(f"L·ªói khi s·ª≠ d·ª•ng Selenium: {e}")
            return None
    
    def get_page_content_requests(self, url, params=None):
        """L·∫•y n·ªôi dung trang web b·∫±ng requests"""
        try:
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f"L·ªói khi truy c·∫≠p {url}: {e}")
            return None
    
    def analyze_page_structure(self, html_content):
        """Ph√¢n t√≠ch c·∫•u tr√∫c trang ƒë·ªÉ t√¨m d·ªØ li·ªáu PSU"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        print("=== PH√ÇN T√çCH C·∫§U TR√öC TRANG ===")
        
        # T√¨m t·∫•t c·∫£ c√°c table
        tables = soup.find_all('table')
        print(f"S·ªë l∆∞·ª£ng table t√¨m th·∫•y: {len(tables)}")
        
        for i, table in enumerate(tables):
            print(f"\nTable {i+1}:")
            rows = table.find_all('tr')
            print(f"  - S·ªë d√≤ng: {len(rows)}")
            if rows:
                first_row = rows[0]
                cells = first_row.find_all(['th', 'td'])
                print(f"  - S·ªë c·ªôt: {len(cells)}")
                if cells:
                    headers = [cell.get_text(strip=True)[:50] for cell in cells[:5]]  # L·∫•y 5 c·ªôt ƒë·∫ßu
                    print(f"  - Headers m·∫´u: {headers}")
        
        # T√¨m c√°c div c√≥ th·ªÉ ch·ª©a d·ªØ li·ªáu
        divs_with_class = soup.find_all('div', class_=True)
        class_names = {}
        for div in divs_with_class:
            for class_name in div.get('class', []):
                if any(keyword in class_name.lower() for keyword in ['table', 'grid', 'list', 'data', 'psu']):
                    class_names[class_name] = class_names.get(class_name, 0) + 1
        
        if class_names:
            print(f"\nC√°c class c√≥ th·ªÉ ch·ª©a d·ªØ li·ªáu: {class_names}")
        
        # T√¨m c√°c element c√≥ id li√™n quan
        elements_with_id = soup.find_all(attrs={'id': True})
        relevant_ids = []
        for element in elements_with_id:
            element_id = element.get('id', '')
            if any(keyword in element_id.lower() for keyword in ['table', 'grid', 'list', 'data', 'psu']):
                relevant_ids.append(element_id)
        
        if relevant_ids:
            print(f"C√°c ID c√≥ th·ªÉ ch·ª©a d·ªØ li·ªáu: {relevant_ids}")
        
        # T√¨m script tags c√≥ th·ªÉ ch·ª©a d·ªØ li·ªáu JSON
        scripts = soup.find_all('script')
        print(f"\nS·ªë script tags: {len(scripts)}")
        for i, script in enumerate(scripts):
            if script.string and any(keyword in script.string.lower() for keyword in ['psu', 'data', 'json']):
                print(f"Script {i+1} c√≥ th·ªÉ ch·ª©a d·ªØ li·ªáu PSU")
    
    def extract_psu_data_advanced(self, html_content):
        """Tr√≠ch xu·∫•t d·ªØ li·ªáu PSU v·ªõi nhi·ªÅu ph∆∞∆°ng ph√°p kh√°c nhau"""
        soup = BeautifulSoup(html_content, 'html.parser')
        psu_data = []
        
        # Ph∆∞∆°ng ph√°p 1: T√¨m trong script tags (JSON data)
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string:
                # T√¨m JSON data trong script
                json_matches = re.findall(r'(\[.*?\]|\{.*?\})', script.string, re.DOTALL)
                for match in json_matches:
                    try:
                        data = json.loads(match)
                        if isinstance(data, list) and len(data) > 0:
                            # Ki·ªÉm tra xem c√≥ ph·∫£i d·ªØ li·ªáu PSU kh√¥ng
                            first_item = data[0]
                            if isinstance(first_item, dict):
                                keys = list(first_item.keys())
                                if any(keyword in str(keys).lower() for keyword in ['psu', 'model', 'efficiency', 'power']):
                                    print(f"T√¨m th·∫•y d·ªØ li·ªáu JSON trong script: {len(data)} items")
                                    return data
                    except:
                        continue
        
        # Ph∆∞∆°ng ph√°p 2: T√¨m trong c√°c table
        tables = soup.find_all('table')
        for table in tables:
            table_data = self.parse_table(table)
            if table_data:
                psu_data.extend(table_data)
        
        # Ph∆∞∆°ng ph√°p 3: T√¨m trong c√°c div c√≥ c·∫•u tr√∫c gi·ªëng grid
        grid_divs = soup.find_all('div', class_=re.compile(r'grid|table|list', re.I))
        for div in grid_divs:
            div_data = self.parse_div_grid(div)
            if div_data:
                psu_data.extend(div_data)
        
        return psu_data
    
    def parse_table(self, table):
        """Parse d·ªØ li·ªáu t·ª´ table HTML"""
        rows = table.find_all('tr')
        if len(rows) < 2:  # C·∫ßn √≠t nh·∫•t header v√† 1 d√≤ng d·ªØ li·ªáu
            return []
        
        # L·∫•y headers
        header_row = rows[0]
        headers = []
        for cell in header_row.find_all(['th', 'td']):
            headers.append(cell.get_text(strip=True))
        
        if not headers:
            return []
        
        # L·∫•y d·ªØ li·ªáu
        data = []
        for row in rows[1:]:
            cells = row.find_all(['td', 'th'])
            if len(cells) == 0:
                continue
                
            row_data = {}
            for i, cell in enumerate(cells):
                header = headers[i] if i < len(headers) else f"Column_{i}"
                text = cell.get_text(strip=True)
                
                # T√¨m links
                links = cell.find_all('a')
                if links:
                    row_data[f"{header}_links"] = [urljoin(self.base_url, link.get('href', '')) for link in links if link.get('href')]
                
                row_data[header] = text
            
            if any(row_data.values()):  # Ch·ªâ th√™m n·∫øu c√≥ d·ªØ li·ªáu
                data.append(row_data)
        
        return data
    
    def parse_div_grid(self, div):
        """Parse d·ªØ li·ªáu t·ª´ div c√≥ c·∫•u tr√∫c grid"""
        # T√¨m c√°c element con c√≥ th·ªÉ ch·ª©a d·ªØ li·ªáu PSU
        items = div.find_all(['div', 'span', 'p'], class_=re.compile(r'item|row|card', re.I))
        if not items:
            return []
        
        data = []
        for item in items:
            item_data = {}
            text = item.get_text(strip=True)
            if text and len(text) > 10:  # B·ªè qua text qu√° ng·∫Øn
                item_data['content'] = text
                
                # T√¨m links
                links = item.find_all('a')
                if links:
                    item_data['links'] = [urljoin(self.base_url, link.get('href', '')) for link in links if link.get('href')]
                
                data.append(item_data)
        
        return data
    
    def scrape_psu_database(self):
        """Scrape d·ªØ li·ªáu PSU database"""
        print("B·∫Øt ƒë·∫ßu scrape d·ªØ li·ªáu PSU...")
        
        # L·∫•y n·ªôi dung trang
        if self.use_selenium:
            html_content = self.get_page_content_selenium(self.database_url)
        else:
            html_content = self.get_page_content_requests(self.database_url)
        
        if not html_content:
            print("Kh√¥ng th·ªÉ l·∫•y n·ªôi dung trang")
            return []
        
        # Ph√¢n t√≠ch c·∫•u tr√∫c trang
        self.analyze_page_structure(html_content)
        
        # Tr√≠ch xu·∫•t d·ªØ li·ªáu
        psu_data = self.extract_psu_data_advanced(html_content)
        
        return psu_data
    
    def save_data(self, data, filename_prefix="cybenetics_psu_data"):
        """L∆∞u d·ªØ li·ªáu ra file"""
        if not data:
            print("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u")
            return None, None
        
        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
        os.makedirs('output', exist_ok=True)
        
        # L∆∞u ra JSON tr∆∞·ªõc
        json_file = f"output/{filename_prefix}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ƒê√£ l∆∞u d·ªØ li·ªáu ra file JSON: {json_file}")
        
        # L∆∞u ra CSV n·∫øu c√≥ th·ªÉ
        try:
            df = pd.DataFrame(data)
            csv_file = f"output/{filename_prefix}.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8')
            print(f"ƒê√£ l∆∞u {len(data)} PSU ra file CSV: {csv_file}")
            return csv_file, json_file
        except Exception as e:
            print(f"Kh√¥ng th·ªÉ l∆∞u CSV: {e}")
            return None, json_file
    
    def close(self):
        """ƒê√≥ng WebDriver n·∫øu ƒëang s·ª≠ d·ª•ng"""
        if self.use_selenium and hasattr(self, 'driver'):
            self.driver.quit()

def main():
    """H√†m main ƒë·ªÉ ch·∫°y scraper"""
    print("=== CYBENETICS PSU SCRAPER ===")
    print("Ch·ªçn ph∆∞∆°ng ph√°p scraping:")
    print("1. S·ª≠ d·ª•ng Selenium (khuy·∫øn ngh·ªã, x·ª≠ l√Ω ƒë∆∞·ª£c JavaScript)")
    print("2. S·ª≠ d·ª•ng requests (nhanh h∆°n nh∆∞ng kh√¥ng x·ª≠ l√Ω JavaScript)")
    
    choice = input("Nh·∫≠p l·ª±a ch·ªçn (1 ho·∫∑c 2): ").strip()
    use_selenium = choice == "1"
    
    if use_selenium:
        print("\nL∆∞u √Ω: C·∫ßn c√†i ƒë·∫∑t ChromeDriver ƒë·ªÉ s·ª≠ d·ª•ng Selenium")
        print("T·∫£i ChromeDriver t·∫°i: https://chromedriver.chromium.org/")
    
    scraper = CybeneticsPSUScraper(use_selenium=use_selenium)
    
    try:
        # Scrape d·ªØ li·ªáu
        psu_data = scraper.scrape_psu_database()
        
        if psu_data:
            print(f"\n‚úÖ ƒê√£ scrape th√†nh c√¥ng {len(psu_data)} items")
            
            # Hi·ªÉn th·ªã m·∫´u d·ªØ li·ªáu
            print("\n--- M·∫™U D·ªÆ LI·ªÜU ---")
            for i, item in enumerate(psu_data[:3]):
                print(f"\nItem {i+1}:")
                for key, value in item.items():
                    if isinstance(value, str) and len(value) > 100:
                        print(f"  {key}: {value[:100]}...")
                    else:
                        print(f"  {key}: {value}")
            
            # L∆∞u d·ªØ li·ªáu
            csv_file, json_file = scraper.save_data(psu_data)
            
            print(f"\nüìä TH·ªêNG K√ä:")
            print(f"- T·ªïng s·ªë items: {len(psu_data)}")
            print(f"- File JSON: {json_file}")
            if csv_file:
                print(f"- File CSV: {csv_file}")
            
        else:
            print("‚ùå Kh√¥ng scrape ƒë∆∞·ª£c d·ªØ li·ªáu n√†o")
            print("C√≥ th·ªÉ trang web s·ª≠ d·ª•ng JavaScript ho·∫∑c c√≥ c·∫•u tr√∫c ph·ª©c t·∫°p")
            print("H√£y th·ª≠ s·ª≠ d·ª•ng Selenium ho·∫∑c ki·ªÉm tra l·∫°i URL")
    
    finally:
        scraper.close()

if __name__ == "__main__":
    main()